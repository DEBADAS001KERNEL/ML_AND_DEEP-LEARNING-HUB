# -*- coding: utf-8 -*-
"""Simple Rainfall Classification .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lTz3rv0qFGO10K14bvdU2pcEZ4OmnCtY
"""

## Introduction to Classification Problems and Rainfall Prediction Project

#Classification problems are a fundamental type of machine learning task where the goal is to assign input data points to one or more predefined categories or classes. In this project, we will be delving into the realm of rainfall prediction, where we aim to build a classifier capable of accurately predicting whether it will rain or not based on various weather-related features.

## Project Overview

#Our rainfall prediction project will utilize a dataset containing historical weather data, including features such as temperature, humidity, wind speed, and atmospheric pressure. The objective is to train a classification model on this data, which can then be used to predict the likelihood of rainfall for new, unseen weather conditions.

## Benefits and Applications

#Accurately predicting rainfall has numerous benefits and practical applications. It can assist farmers in making informed decisions about crop selection and irrigation schedules, enabling them to optimize their agricultural practices. Additionally, rainfall predictions are crucial for disaster management, helping authorities prepare for and mitigate the impacts of heavy rainfall events.

## Our Approach

#In this project, we will explore various classification algorithms such as logistic regression, decision trees, and support vector machines to identify the most suitable model for our rainfall prediction task. We will evaluate the performance of each algorithm based on its accuracy, precision, and recall metrics.

## Conclusion

#By successfully completing this project, we aim to develop a rainfall prediction model that can provide valuable insights and aid in decision-making processes related to agriculture, disaster management, and other rainfall-sensitive sectors.

# here im going to use this data set
"https://www.kaggle.com/datasets/sujithmandala/simple-rainfall-classification-dataset?select=rainfall.csv"

# importing important modules

import pandas as pd
import tensorflow as tf
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import  MinMaxScaler,LabelEncoder
import matplotlib.pyplot as plt



# let's see the data
data=pd.read_csv("/content/rainfall.csv")
data.head()

# check the shape
data.shape

data.info()



"""**We have downloaded this dataset, which is already in a clean version, making it a good candidate for building our first machine learning model. While we can perform additional Feature Engineering (FE) and Exploratory Data Analysis (EDA) to gain deeper insights and potentially improve model performance, we will focus on building a basic model first. These additional steps can be explored in future projects as we gain more experience. This approach allows us to get hands-on practice with model building without getting overwhelmed by the complexities of data analysis.**

**here we can see we have  NAN values in row no 53, lets remove the row**
"""

data

# so friends , i think ,we can  remove the "date" column cause it doesn't provide any predictive power for your model.

'''
SOME IMPORTANT REASONS TO REMOVE "date" COLOUMN

1. Lack of Direct Influence

The date itself does not inherently influence weather conditions like rainfall, temperature,
 humidity, or wind speed. These weather parameters are more directly influenced by atmospheric conditions rather
 than the specific date.

2. Temporal Patterns Not Captured

Weather conditions do have seasonal patterns, but these patterns are better captured by features derived from the date,
 such as the month or season. The raw date column does not explicitly convey this information.

3. Irrelevant Information

The date is simply a timestamp and does not contain meaningful information about the weather.
It acts more as an identifier rather than a feature with predictive power.

4. Feature Engineering
Relevant temporal patterns should be captured through feature engineering.
 For instance, extracting the month or season from the date could help capture seasonal variations in weather, but
 using the raw date as is won't contribute to the model's performance.

5. Overfitting Risk

Including the date column as a feature might lead to overfitting, where the model learns
the noise in the training data rather than the underlying pattern.
 This reduces the model's ability to generalize to new data.

6. High Cardinality
Dates have high cardinality (many unique values) which can complicate the
model without adding value. Most machine learning models do
not handle high cardinality
features well without appropriate transformation.


'''
data.drop('date', axis=1, inplace=True)

data.shape

print("\nChecking for NaN values:")
print(data.isna())

# Remove rows with NaN values
data = data.dropna()

print("\nDataFrame after removing rows with NaN values:")
print(data)

label_encoder = LabelEncoder()

data['weather_condition'] = label_encoder.fit_transform(data['weather_condition'])

data.head()

data.shape

print("\nChecking for NaN values:")
print(data.isna())

# Remove rows with NaN values
cleaned_data = data.dropna()

print("\nDataFrame after removing rows with NaN values:")
print(cleaned_data)

from sklearn.compose import make_column_transformer


# Create column transformer (this will help us normalize/preprocess our data)
ct = make_column_transformer(
    (MinMaxScaler(), ["rainfall", "temperature", "humidity","wind_speed"]), # get all values between 0 and 1

)

# Create X & y
x = data.drop("weather_condition", axis=1)
y = data["weather_condition"]

from sklearn.model_selection import train_test_split
# Build our train and test sets (use random state to ensure same split as before)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Fit column transformer on the training data only (doing so on test data would result in data leakage)
ct.fit(x_train)

# Transform training and test data with normalization (MinMaxScalar) and one hot encoding (OneHotEncoder)
x_train_normal = ct.transform(x_train)
x_test_normal = ct.transform(x_test)

print(X_train_normal)

print(y)

# buield the model

model_1=tf.keras.Sequential([
    tf.keras.layers.Dense(4,activation="relu"),

    tf.keras.layers.Dense(1,activation="sigmoid"),
])

model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
                metrics=["accuracy"])

hist=model_1.fit(x_train_normal,y_train,epochs=200,validation_data=(x_test_normal,y_test))

model_1.evaluate(x_test_normal,y_test)

import matplotlib.pyplot as plt
pd.DataFrame(hist.history).plot()

loss, accuracy = model_1.evaluate(x_test_normal, y_test, verbose=0)
print(f'Validation Accuracy: {accuracy:.4f}')

y_pred=model_1.predict(x_test_normal)

from sklearn.metrics import accuracy_score


# Evaluate the model on the test data
y_test_pred = model_1.predict(x_test_normal)
y_test_pred_classes = (y_test_pred > 0.5).astype(int)

# Calculate the accuracy score
test_accuracy = accuracy_score(y_test, y_test_pred_classes)
print(f'Test Accuracy: {test_accuracy:.4f}')

from sklearn.metrics import accuracy_score, precision_score, recall_score
precision = precision_score(y_test, y_test_pred_classes)
recall = recall_score(y_test, y_test_pred_classes)
print(f'Test Precision: {precision:.4f}')
print(f'Test Recall: {recall:.4f}')

# as we can see our model is working perfectly.